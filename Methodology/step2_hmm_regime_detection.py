"""
This is the new code

Complete Integrated Pipeline for Options Trading Research
Just run this file and everything executes automatically
"""

import psycopg2
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from datetime import datetime
import warnings
from scipy.stats import norm
from hmmlearn import hmm
import pickle
from step1_redone_filtering import OptionsDataFilter, get_clean_options_data



warnings.filterwarnings('ignore')

print("""
‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó
‚ïë   OPTIONS TRADING RESEARCH - REGIME-BASED PRICING ANALYSIS     ‚ïë
‚ïë                                                                ‚ïë
‚ïë   Exploiting Volatility Surface Inefficiencies Through         ‚ïë
‚ïë   Stochastic Control and Monte Carlo Validation                ‚ïë
‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù
""")

# ================================
# CONFIGURATION
# ================================
SIMPLE = False  # Set to True for quick testing with limited data


# Database connection parameters
DB_CONFIG = {
    "host": "localhost",
    "port": "5432",
    "database": "options_data",
    "user": "postgres",
    "password": "postgres"
}

# HMM Configuration
N_REGIMES = 4  # Bull/Low-vol, Bear/High-vol, High-vol, Low-vol
WINDOW_LENGTHS = [20, 60, 120, 252]  # Trading days for sensitivity analysis
MAX_ITERATIONS = 1000  # For Baum-Welch algorithm
CONVERGENCE_TOL = 1e-4  # Convergence tolerance


# ================================
# STEP 1: DATA EXTRACTION & PREPARATION
# ================================

def connect_to_db():
    """Establish connection to PostgreSQL database"""
    try:
        conn = psycopg2.connect(**DB_CONFIG)
        print("‚úì Successfully connected to database")
        return conn
    except Exception as e:
        print(f"‚úó Database connection failed: {e}")
        return None


def extract_options_data(conn, limit=None):
    """Extract options data from database"""


    print("\n" + "=" * 60)
    print("EXTRACTING OPTIONS DATA")
    print("=" * 60)

    query = """
    SELECT
        asofdate,
        (data->'attributes'->>'strike')::float AS strike,
        (data->'attributes'->>'exp_date') AS exp_date,
        (data->'attributes'->>'type') AS option_type,
        (data->'attributes'->>'bid')::float AS bid,
        (data->'attributes'->>'ask')::float AS ask,
        (data->'attributes'->>'last')::float AS last_price,
        (data->'attributes'->>'midpoint')::float AS mid_price,
        (data->'attributes'->>'volume')::float AS volume,
        (data->'attributes'->>'open_interest')::float AS open_interest,
        (data->'attributes'->>'volatility')::float AS volatility,
        (data->'attributes'->>'underlying_symbol') AS underlying_symbol,
        (data->'attributes'->>'moneyness')::float AS moneyness,
        (data->'attributes'->>'delta')::float AS delta,
        (data->'attributes'->>'gamma')::float AS gamma,
        (data->'attributes'->>'theta')::float AS theta,
        (data->'attributes'->>'vega')::float AS vega,
        (data->'attributes'->>'dte')::float AS days_to_exp
    FROM options
    WHERE (data->'attributes'->>'underlying_symbol') IN ('SPY', 'QQQ', 'IWM', 'AAPL', 'MSFT', 'TSLA', 'XOM', 'JPM')
    ORDER BY asofdate, exp_date, strike
    """

    if limit:
        query += f" LIMIT {limit}"

    print(f"Querying database... (limit: {limit if limit else 'None'})")
    df = pd.read_sql(query, conn)
    print(f"‚úì Extracted {len(df):,} records")
    return df

def prepare_options_data(df, db_connection):
    """
    Clean and prepare options data using Step 1 filtering

    Uses OptionsDataFilter from step1_redone_filtering.py
    """
    print("\n" + "=" * 60)
    print("PREPARING OPTIONS DATA - USING STEP 1 FILTERS")
    print("=" * 60)

    # Initialize filter WITH database connection
    filter_obj = OptionsDataFilter(db_connection, verbose=True)

    # Apply standardized filters (will fetch from stocks table)
    df_clean = filter_obj.apply_filters(df)

    # Add moneyness classification (keep this custom logic)
    def classify_moneyness(row):
        m = row['moneyness_pct']
        if pd.isna(m):
            return None
        if row['option_type'] == 'call':
            if m < -0.02:
                return 'ITM'
            elif m > 0.02:
                return 'OTM'
            else:
                return 'ATM'
        else:  # put
            if m > 0.02:
                return 'ITM'
            elif m < -0.02:
                return 'OTM'
            else:
                return 'ATM'

    df_clean['moneyness_class'] = df_clean.apply(classify_moneyness, axis=1)

    print(f"\n‚úì Data preparation complete")
    print(f"  Date range: {df_clean['asofdate'].min().date()} ‚Üí {df_clean['asofdate'].max().date()}")
    print(f"  Symbols: {df_clean['underlying_symbol'].nunique()}")
    print(f"  Calls: {(df_clean['option_type'] == 'call').sum():,}")
    print(f"  Puts: {(df_clean['option_type'] == 'put').sum():,}")

    return df_clean



# ================================
# STEP 2: HMM REGIME DETECTION
# ================================
"""
Fixed HMM Regime Detection with Numerical Stability
"""

import numpy as np
from hmmlearn import hmm
import pandas as pd

"""
Fixed HMM Regime Detection with Numerical Stability
"""

import numpy as np
from hmmlearn import hmm
import pandas as pd

"""
Fixed HMM Regime Detection with Numerical Stability
"""

import numpy as np
from hmmlearn import hmm
import pandas as pd


class RegimeDetector:
    """Hidden Markov Model for regime detection with numerical stability fixes"""

    def __init__(self, n_regimes=4, max_iter=1000, tol=1e-4):
        self.n_regimes = n_regimes
        self.max_iter = max_iter
        self.tol = tol
        self.model = None
        self.regime_params = None
        self.regime_labels = None

    def prepare_features(self, returns_df, window_length):
        """Prepare features for HMM"""
        print(f"\nPreparing features with {window_length}-day window...")

        features_list = []

        for symbol in returns_df['underlying_symbol'].unique():
            symbol_data = returns_df[returns_df['underlying_symbol'] == symbol].copy()
            symbol_data = symbol_data.sort_values('asofdate')

            # Calculate realized volatility with minimum periods check
            min_periods = max(20, window_length // 2)  # More robust minimum

            symbol_data['realized_vol'] = symbol_data['log_return'].rolling(
                window=window_length, min_periods=min_periods
            ).std() * np.sqrt(252)

            symbol_data['rolling_mean'] = symbol_data['log_return'].rolling(
                window=window_length, min_periods=min_periods
            ).mean() * 252

            features_list.append(symbol_data)

        features_df = pd.concat(features_list, ignore_index=True)
        features_df = features_df.dropna(subset=['realized_vol', 'rolling_mean'])

        # Remove any infinite or extremely large values
        features_df = features_df[np.isfinite(features_df['realized_vol'])]
        features_df = features_df[np.isfinite(features_df['rolling_mean'])]
        features_df = features_df[features_df['realized_vol'] > 0]  # Ensure positive volatility

        print(f"  Features prepared: {len(features_df):,} observations")
        return features_df

    def fit(self, returns_df, window_length):
        """Fit HMM using Baum-Welch algorithm with numerical stability"""
        print(f"\n{'=' * 60}")
        print(f"FITTING HMM WITH {window_length}-DAY WINDOW")
        print(f"{'=' * 60}")

        features_df = self.prepare_features(returns_df, window_length)

        # Check if we have enough data
        if len(features_df) < self.n_regimes * 50:
            print(f"‚ö† Warning: Limited data ({len(features_df)} obs) for {self.n_regimes} regimes")
            print(f"  Recommended: at least {self.n_regimes * 50} observations")

        # Prepare feature matrix with standardization
        X_raw = np.column_stack([
            features_df['log_return'].values,
            features_df['realized_vol'].values
        ])

        # Standardize features to improve numerical stability
        X_mean = X_raw.mean(axis=0)
        X_std = X_raw.std(axis=0)
        X_std[X_std == 0] = 1.0  # Avoid division by zero
        X = (X_raw - X_mean) / X_std

        # Add small jitter to break any exact duplicates
        X = X + np.random.normal(0, 1e-8, X.shape)

        print(f"\nTraining HMM with {self.n_regimes} regimes...")
        print(f"  Feature matrix shape: {X.shape}")
        print(f"  Feature ranges: returns [{X[:, 0].min():.4f}, {X[:, 0].max():.4f}], "
              f"vol [{X[:, 1].min():.4f}, {X[:, 1].max():.4f}]")

        # Initialize model with diagonal covariance (more stable)
        self.model = hmm.GaussianHMM(
            n_components=self.n_regimes,
            covariance_type="diag",  # Changed from "full" to "diag" for stability
            n_iter=self.max_iter,
            tol=self.tol,
            random_state=42,
            min_covar=1e-3,  # Prevent covariance collapse
            init_params="stmc",  # Initialize all parameters
            params="stmc"  # Update all parameters
        )

        # Provide better initial parameters
        self._initialize_params(X)

        try:
            self.model.fit(X)
            print(f"‚úì Model converged after {self.model.monitor_.iter} iterations")
            print(f"  Final log-likelihood: {self.model.monitor_.history[-1]:.2f}")
        except Exception as e:
            print(f"‚úó Model fitting failed: {e}")
            print("  Attempting fallback to simpler initialization...")

            # Fallback: Use k-means initialization
            from sklearn.cluster import KMeans
            kmeans = KMeans(n_clusters=self.n_regimes, random_state=42, n_init=10)
            labels = kmeans.fit_predict(X)

            # Set initial means from k-means
            self.model.means_ = kmeans.cluster_centers_

            # Set uniform covariances
            self.model.covars_ = np.ones((self.n_regimes, X.shape[1])) * 0.5

            try:
                self.model.fit(X)
                print(f"‚úì Fallback successful - converged after {self.model.monitor_.iter} iterations")
            except Exception as e2:
                print(f"‚úó Fallback also failed: {e2}")
                raise

        # Predict regimes
        regimes = self.model.predict(X)
        features_df['regime'] = regimes

        # Unstandardize the features for parameter extraction
        features_df['log_return_unstd'] = features_df['log_return']
        features_df['realized_vol_unstd'] = features_df['realized_vol']

        self._extract_regime_parameters(features_df)
        self._label_regimes()

        return features_df

    def _initialize_params(self, X):
        """Initialize HMM parameters more carefully"""
        from sklearn.cluster import KMeans

        # Use k-means for better initialization
        kmeans = KMeans(n_clusters=self.n_regimes, random_state=42, n_init=10)
        labels = kmeans.fit_predict(X)

        # Set initial means
        self.model.means_ = kmeans.cluster_centers_

        # Set initial covariances based on cluster variance
        covars = []
        for k in range(self.n_regimes):
            cluster_data = X[labels == k]
            if len(cluster_data) > 1:
                var = np.var(cluster_data, axis=0)
                var = np.maximum(var, 1e-3)  # Ensure minimum variance
            else:
                var = np.ones(X.shape[1]) * 0.5
            covars.append(var)

        self.model.covars_ = np.array(covars)

        # Set uniform transition matrix initially
        self.model.transmat_ = np.ones((self.n_regimes, self.n_regimes)) / self.n_regimes
        self.model.transmat_ += np.eye(self.n_regimes) * 0.5  # Add self-transition bias
        self.model.transmat_ = self.model.transmat_ / self.model.transmat_.sum(axis=1, keepdims=True)

        # Set uniform start probabilities
        self.model.startprob_ = np.ones(self.n_regimes) / self.n_regimes

    def _extract_regime_parameters(self, features_df):
        """Extract regime parameters"""
        print("\nExtracting regime parameters...")

        self.regime_params = {}

        for regime_idx in range(self.n_regimes):
            regime_data = features_df[features_df['regime'] == regime_idx]

            if len(regime_data) > 0:
                mu = regime_data['log_return_unstd'].mean() * 252
                sigma = regime_data['log_return_unstd'].std() * np.sqrt(252)

                # Handle potential NaN values
                if np.isnan(sigma) or sigma == 0:
                    sigma = 0.01  # Small default value

                threshold = 2 * regime_data['log_return_unstd'].std()
                if np.isnan(threshold) or threshold == 0:
                    threshold = 0.02

                jumps = np.abs(regime_data['log_return_unstd']) > threshold
                lambda_j = jumps.sum() / len(regime_data) * 252

                self.regime_params[regime_idx] = {
                    'mu': mu,
                    'sigma': sigma,
                    'lambda': lambda_j,
                    'n_observations': len(regime_data)
                }

                print(f"  Regime {regime_idx}: Œº={mu:.4f}, œÉ={sigma:.4f}, Œª={lambda_j:.4f}, n={len(regime_data)}")
            else:
                print(f"  Regime {regime_idx}: No observations (empty regime)")

    # ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî REPLACE THIS FUNCTION ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî

    def _label_regimes(self):
        """
        FIXED: Rank-based labeling to ensure all 4 unique regime labels

        Strategy: Sort by drift, then by volatility within each half
        - Top 2 drift ‚Üí Bull
        - Bottom 2 drift ‚Üí Bear
        - Within each pair, higher vol ‚Üí High-Vol, lower vol ‚Üí Low-Vol
        """
        if not self.regime_params:
            self.regime_labels = {i: f"Regime {i}" for i in range(self.n_regimes)}
            return

        # Collect regime stats
        regimes = []
        for idx, p in self.regime_params.items():
            regimes.append({
                'idx': idx,
                'mu': p['mu'],
                'sigma': p['sigma'],
                'n': p.get('n_observations', 0)
            })

        # Sort by drift (descending) - top 2 are Bull, bottom 2 are Bear
        regimes_by_drift = sorted(regimes, key=lambda x: x['mu'], reverse=True)

        # Split into Bull (top 2) and Bear (bottom 2)
        bull_regimes = regimes_by_drift[:2]
        bear_regimes = regimes_by_drift[2:]

        # Within Bull: higher vol = High-Vol, lower vol = Low-Vol
        bull_regimes = sorted(bull_regimes, key=lambda x: x['sigma'], reverse=True)

        # Within Bear: higher vol = High-Vol, lower vol = Low-Vol
        bear_regimes = sorted(bear_regimes, key=lambda x: x['sigma'], reverse=True)

        labels = {}

        # Assign Bull labels
        labels[bull_regimes[0]['idx']] = "Bull/High-Vol"
        labels[bull_regimes[1]['idx']] = "Bull/Low-Vol"

        # Assign Bear labels
        labels[bear_regimes[0]['idx']] = "Bear/High-Vol"
        labels[bear_regimes[1]['idx']] = "Bear/Low-Vol"

        print("\nRegime Classification (rank-based, all 4 unique):")
        print("-" * 70)
        print(f"{'Regime':<8} {'Label':<18} {'Drift (Œº)':>12} {'Vol (œÉ)':>12} {'Obs':>10}")
        print("-" * 70)

        for r in regimes_by_drift:
            idx = r['idx']
            label = labels[idx]
            print(f"  {idx:<6} {label:<18} {r['mu']:>+12.4f} {r['sigma']:>12.4f} {r['n']:>10,}")

        print("-" * 70)

        self.regime_labels = labels

    def get_transition_matrix(self):  # <-- This is OUTSIDE the class!
        """Get transition matrix"""
        if self.model is None:
            return None
        return self.model.transmat_

# ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî AND REPLACE sensitivity_analysis() ‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî‚Äî
def sensitivity_analysis(returns_df, window_lengths, n_regimes=4):
    """Now works even if only 1 window succeeds"""
    print("\n" + "="*60)
    print("SENSITIVITY ANALYSIS: WINDOW LENGTH")
    print("="*60)

    results = {}
    successful_windows = []

    for window in window_lengths:
        print(f"\nFITTING HMM WITH {window}-DAY WINDOW")
        try:
            detector = RegimeDetector(n_regimes=n_regimes)
            features_df = detector.fit(returns_df, window)

            # Force labeling even if partial
            if detector.regime_labels is None:
                detector._label_regimes()

            regime_changes = (features_df['regime'] != features_df['regime'].shift(1)).sum()
            avg_duration = len(features_df) / max(regime_changes, 1)

            results[window] = {
                'detector': detector,
                'features_df': features_df,
                'avg_duration': avg_duration,
                'success': True
            }
            successful_windows.append(window)
            print(f"Window {window}: Avg duration = {avg_duration:.1f} days")

        except Exception as e:
            print(f"Window {window} failed: {e}")
            results[window] = {'success': False, 'error': str(e)}

    if not successful_windows:
        raise ValueError("All windows failed. Try n_regimes=3 or check data.")

    # Pick longest duration UNDER 100 days, otherwise pick the longest that worked
    candidates = [w for w in successful_windows if results[w]['avg_duration'] < 100]
    if candidates:
        optimal_window = max(candidates, key=lambda w: results[w]['avg_duration'])
    else:
        optimal_window = max(successful_windows, key=lambda w: results[w]['avg_duration'])

    print(f"\nOPTIMAL WINDOW SELECTED: {optimal_window} days")
    return results, optimal_window

# ================================
# UPDATED: visualize_regimes - FIXED DATA MERGING
# ================================

# ===================================================================
# 3. UPDATE: visualize_regimes() ‚Üí now shows the train/test split line
# ===================================================================
def visualize_regimes(features_df, detector, underlying_df, window_length, train_end_date=None):
    """Training-period visualization ‚Äî FINAL, UNBREAKABLE VERSION"""
    print("\nCreating TRAINING PERIOD regime visualization...")

    symbol = 'SPY' if 'SPY' in features_df['underlying_symbol'].unique() else features_df['underlying_symbol'].iloc[0]
    print(f"   Plotting regimes for: {symbol}")

    feat = features_df[features_df['underlying_symbol'] == symbol][['asofdate', 'regime']].copy()
    price = underlying_df[underlying_df['underlying_symbol'] == symbol][['asofdate', 'underlying_price']].copy()

    spy_data = feat.merge(price, on='asofdate', how='left')

    if 'underlying_price' not in spy_data.columns or spy_data['underlying_price'].isna().all():
        print(f"   No price data for {symbol} ‚Äî skipping")
        return None

    spy_data = spy_data.dropna(subset=['underlying_price', 'regime'])

    # Ensure asofdate is datetime (not string)
    spy_data['asofdate'] = pd.to_datetime(spy_data['asofdate'])

    # Convert train_end_date to datetime if needed
    if train_end_date is not None:
        train_end_dt = pd.to_datetime(train_end_date)
    else:
        train_end_dt = None

    regime_colors = {
        'Bull/High-Vol': '#d62728',
        'Bull/Low-Vol': '#2ca02c',
        'Bear/High-Vol': '#ff7f0e',
        'Bear/Low-Vol': '#1f77b4',
    }

    fig = go.Figure()

    for regime_idx in sorted(spy_data['regime'].unique()):
        mask = spy_data['regime'] == regime_idx
        label = detector.regime_labels.get(regime_idx, f"Regime {regime_idx}")
        color = regime_colors.get(label, 'gray')

        fig.add_trace(go.Scatter(
            x=spy_data[mask]['asofdate'],  # Keep as datetime
            y=spy_data[mask]['underlying_price'],
            mode='lines',
            name=label,
            line=dict(color=color, width=3),
            hovertemplate='<b>%{text}</b><br>Date: %{x}<br>Price: $%{y:.2f}<extra></extra>',
            text=[label] * mask.sum()
        ))

    # Now add the vline with datetime object
    if train_end_dt is not None:
        fig.add_vline(
            x=train_end_dt.timestamp() * 1000,  # Convert to milliseconds timestamp
            line=dict(color="white", width=6, dash="dash"),
            annotation_text="TRAIN ‚Üí TEST<br>(Options Start)",
            annotation_position="top right",
            annotation=dict(font_size=16, font_color="white", bgcolor="black")
        )
        fig.add_vrect(
            x0=spy_data['asofdate'].min(),
            x1=train_end_dt.timestamp() * 1000,
            fillcolor="rgba(100,100,100,0.2)",
            layer="below",
            line_width=0
        )

    fig.update_layout(
        title=f"<b>{symbol}</b> ‚Äî HMM Regimes (60-day window)<br>Trained 2016 ‚Üí 2024-03-24 | Out-of-Sample Test: 2024-03-25+",
        xaxis_title="Date",
        yaxis_title="Price ($)",
        height=750,
        template="plotly_dark",
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5),
        hovermode="x unified"
    )

    fig.show()
    return fig
# ================================
# UPDATED: visualize_historical_regimes - FIXED DATA MERGING
# ================================
# ===================================================================
# 4. UPDATE: visualize_historical_regimes() ‚Üí beautiful split line + better labels
# ===================================================================
def visualize_historical_regimes(hmm_model_data, returns_df, symbol='SPY', train_end_date=None):
    """Full-history plot with clear train/test demarcation - CORRECTED"""
    print(f"\nCreating FULL HISTORY regime visualization for {symbol}...")

    regime_sequence = hmm_model_data['regime_sequence']
    symbol_reg = regime_sequence[regime_sequence['underlying_symbol'] == symbol].copy()
    symbol_prices = returns_df[returns_df['underlying_symbol'] == symbol][['asofdate', 'underlying_price']].copy()

    merged = symbol_prices.merge(symbol_reg, on='asofdate', how='left')
    merged['regime'] = merged['regime'].ffill()  # propagate regime forward
    merged = merged.dropna(subset=['regime'])
    merged['regime_label'] = merged['regime'].map(hmm_model_data['regime_labels'])

    # Ensure dates are datetime
    merged['asofdate'] = pd.to_datetime(merged['asofdate'])
    if train_end_date is not None:
        train_end_dt = pd.to_datetime(train_end_date)
    else:
        train_end_dt = None

    regime_colors = {
        'Bull/High-Vol': '#d62728',
        'Bull/Low-Vol': '#2ca02c',
        'Bear/High-Vol': '#ff7f0e',
        'Bear/Low-Vol': '#1f77b4',
    }

    fig = go.Figure()

    # CORRECTED: Remove stackgroup and fill - just plot colored line segments
    for label in merged['regime_label'].unique():
        mask = merged['regime_label'] == label
        fig.add_trace(go.Scatter(
            x=merged[mask]['asofdate'],
            y=merged[mask]['underlying_price'],
            mode='lines',
            name=label,
            line=dict(color=regime_colors.get(label, 'gray'), width=3),
            # REMOVED: stackgroup='one' and fill='tonexty'
            hovertemplate='%{x|%b %Y}<br>$%{y:.2f}<br><b>%{text}</b><extra></extra>',
            text=[label] * sum(mask)
        ))

    # <<< TRAIN / TEST SPLIT LINE - ALL IN MILLISECONDS >>>
    if train_end_dt is not None:
        train_end_ms = train_end_dt.timestamp() * 1000
        min_date_ms = merged['asofdate'].min().timestamp() * 1000

        fig.add_vline(
            x=train_end_ms,
            line=dict(color="white", width=4, dash="solid"),
            annotation_text="TRAIN ‚Üí TEST",
            annotation_position="top",
            annotation_font=dict(size=14, color="white"),
            annotation_bgcolor="rgba(0,0,0,0.8)",
            layer="above"
        )
        fig.add_vrect(
            x0=min_date_ms,
            x1=train_end_ms,
            fillcolor="rgba(50,50,50,0.15)",
            layer="below",
            line_width=0,
            annotation_text="Training Period<br>(HMM fitted here)",
            annotation_position="top left"
        )

    fig.update_layout(
        title=f"<b>{symbol}</b> ‚Äî Full History with Out-of-Sample Regimes<br>"
              f"Training: 2016 ‚Üí {train_end_dt.date() if train_end_dt else '?'} | "
              f"Testing (your options data): {merged['asofdate'].min().date()} ‚Üí {merged['asofdate'].max().date()}",
        xaxis_title="Date",
        yaxis_title="Price ($)",
        height=800,
        template="plotly_dark",
        hovermode="x unified",
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5)
    )
    fig.show()
    return fig
def load_hmm_model(filepath='hmm_regime_model.pkl'):
    """
    Load saved HMM model and parameters

    Returns:
    --------
    Dictionary with:
        - transition_matrix: 4x4 transition probabilities
        - regime_params: {regime_idx: {'mu', 'sigma', 'lambda'}}
        - regime_labels: {regime_idx: 'Bull/Low-Vol', etc.}
        - regime_sequence: DataFrame with historical regime classifications
        - hmm_model: trained model for predictions
    """
    with open(filepath, 'rb') as f:
        data = pickle.load(f)

    print("‚úì HMM model loaded")
    print(f"  Optimal window: {data['optimal_window']} days")
    print(f"  Number of regimes: {data['n_regimes']}")
    print(f"  Regime sequence length: {len(data['regime_sequence']):,}")

    return data


def get_current_regime(hmm_model_data, recent_returns, recent_vol):
    """
    Predict current regime given recent market data

    Parameters:
    -----------
    hmm_model_data : dict - loaded from load_hmm_model()
    recent_returns : float - recent log return
    recent_vol : float - recent realized volatility (annualized)

    Returns:
    --------
    regime_idx : int - predicted regime
    regime_label : str - regime name
    probabilities : array - probability distribution over all regimes
    """
    model = hmm_model_data['hmm_model']
    X = np.array([[recent_returns, recent_vol]])

    regime_idx = model.predict(X)[0]
    probabilities = model.predict_proba(X)[0]
    regime_label = hmm_model_data['regime_labels'][regime_idx]

    return regime_idx, regime_label, probabilities


def print_regime_summary(hmm_model_data):
    """
    Print a summary of the regime model
    """
    print("\n" + "="*60)
    print("HMM REGIME MODEL SUMMARY")
    print("="*60)

    print("\nRegime Parameters:")
    for idx, params in hmm_model_data['regime_params'].items():
        label = hmm_model_data['regime_labels'][idx]
        print(f"\n  {label} (Regime {idx}):")
        print(f"    Drift (Œº):          {params['mu']:.4f} ({params['mu']*100:.2f}% annual)")
        print(f"    Volatility (œÉ):     {params['sigma']:.4f} ({params['sigma']*100:.2f}% annual)")
        print(f"    Jump Intensity (Œª): {params['lambda']:.4f} (jumps/year)")
        print(f"    Observations:       {params['n_observations']:,}")

    print("\n" + "="*60)
    print("Transition Matrix P[i,j] = P(regime_t+1 = j | regime_t = i)")
    print("="*60)

    trans_matrix = hmm_model_data['transition_matrix']
    labels = [hmm_model_data['regime_labels'][i] for i in range(len(trans_matrix))]
    trans_df = pd.DataFrame(trans_matrix, index=labels, columns=labels)
    print("\n", trans_df.round(3))

    print("\n" + "="*60)


def calculate_underlying_returns(underlying_df):
    """
    Calculate log returns from underlying prices

    Parameters:
    -----------
    underlying_df : DataFrame with columns ['asofdate', 'underlying_symbol', 'underlying_price']
        This comes from the filtered options data (df_clean['underlying_price'])
    """
    print("\n" + "=" * 60)
    print("CALCULATING UNDERLYING RETURNS")
    print("=" * 60)

    underlying_df = underlying_df.copy()
    underlying_df['asofdate'] = pd.to_datetime(underlying_df['asofdate']).dt.tz_localize(None).dt.normalize()
    underlying_df = underlying_df.sort_values(['underlying_symbol', 'asofdate'])

    # Calculate returns
    underlying_df['log_return'] = underlying_df.groupby('underlying_symbol')['underlying_price'].transform(
        lambda x: np.log(x / x.shift(1))
    )

    returns_df = underlying_df.dropna(subset=['log_return']).copy()

    print(f"‚úì Calculated returns for {returns_df['underlying_symbol'].nunique()} symbols")
    print(f"  Total trading days: {returns_df['asofdate'].nunique()}")
    print(f"  Date range: {returns_df['asofdate'].min().date()} to {returns_df['asofdate'].max().date()}")

    return returns_df


"""
REPLACEMENT FUNCTIONS FOR STEP 2
Use Yahoo Finance non-adjusted close prices instead of database

Add this import at the top of your file:
    import yfinance as yf
"""

import yfinance as yf
import pandas as pd
import numpy as np

def download_underlying_prices_yfinance(start_date, end_date, symbols=None, verbose=True):
    if symbols is None:
        symbols = ['SPY', 'QQQ', 'IWM', 'AAPL', 'MSFT', 'TSLA', 'XOM', 'JPM']

    # ADD THIS LINE:
    if end_date is None:
        end_date = pd.Timestamp.now().strftime('%Y-%m-%d')

    if verbose:
        print(f"Downloading underlying prices from Yahoo Finance...")
        print(f"Date range: {pd.to_datetime(start_date).date()} ‚Üí {pd.to_datetime(end_date).date()}")

    # Add buffer days
    start_dt = pd.to_datetime(start_date) - pd.Timedelta(days=5)
    end_dt = pd.to_datetime(end_date) + pd.Timedelta(days=1)

    all_data = []

    for symbol in symbols:
        try:
            ticker = yf.Ticker(symbol)
            # auto_adjust=False gives non-adjusted close prices
            hist = ticker.history(start=start_dt, end=end_dt, auto_adjust=False)

            if len(hist) == 0:
                if verbose:
                    print(f"    ‚úó {symbol}: No data returned")
                continue

            df = hist[['Close']].reset_index()
            df.columns = ['asofdate', 'underlying_price']
            df['underlying_symbol'] = symbol
            df['asofdate'] = pd.to_datetime(df['asofdate']).dt.tz_localize(None).dt.normalize()

            all_data.append(df[['asofdate', 'underlying_symbol', 'underlying_price']])

            if verbose:
                print(f"    ‚úì {symbol}: {len(df)} days")

        except Exception as e:
            if verbose:
                print(f"    ‚úó {symbol}: Error - {e}")

    if not all_data:
        raise ValueError("No underlying prices downloaded from Yahoo Finance!")

    prices = pd.concat(all_data, ignore_index=True)

    if verbose:
        print(f"‚úì Downloaded {len(prices):,} price observations from Yahoo Finance")

    return prices


def calculate_underlying_returns_yfinance(symbols=None, start_date='2016-01-01', end_date=None, verbose=True):
    """
    Download prices from Yahoo Finance and calculate log returns

    REPLACES: calculate_underlying_returns() when you want longer history

    Parameters:
    -----------
    symbols : list, optional
    start_date : str, default '2016-01-01' for longer training history
    end_date : str, optional (defaults to today)
    verbose : bool

    Returns:
    --------
    pd.DataFrame with columns: asofdate, underlying_symbol, underlying_price, log_return
    """
    if symbols is None:
        symbols = ['SPY', 'QQQ', 'IWM', 'AAPL', 'MSFT', 'TSLA', 'XOM', 'JPM']

    if end_date is None:
        end_date = pd.Timestamp.now().strftime('%Y-%m-%d')

    if verbose:
        print("\n" + "=" * 60)
        print("DOWNLOADING RETURNS FROM YAHOO FINANCE")
        print("=" * 60)

    # Download prices
    prices = download_underlying_prices_yfinance(start_date, end_date, symbols, verbose)

    # Sort and calculate returns
    prices = prices.sort_values(['underlying_symbol', 'asofdate'])

    prices['log_return'] = prices.groupby('underlying_symbol')['underlying_price'].transform(
        lambda x: np.log(x / x.shift(1))
    )

    returns_df = prices.dropna(subset=['log_return']).copy()

    if verbose:
        print(f"\n‚úì Calculated returns for {returns_df['underlying_symbol'].nunique()} symbols")
        print(f"  Total trading days: {returns_df['asofdate'].nunique()}")
        print(f"  Date range: {returns_df['asofdate'].min().date()} ‚Üí {returns_df['asofdate'].max().date()}")

    return returns_df


# ============================================================
# MODIFIED OptionsDataFilter.download_underlying_prices METHOD
# ============================================================
# Add this as a method to your OptionsDataFilter class, or monkey-patch it:

def download_underlying_prices_yf(self, start_date, end_date, symbols=None):
    """
    Replacement method for OptionsDataFilter class
    Downloads from Yahoo Finance instead of database
    """
    return download_underlying_prices_yfinance(
        start_date, end_date,
        symbols=symbols or self.SYMBOLS,
        verbose=self.verbose
    )

# ============================================================
# USAGE IN MAIN() - Replace the returns calculation section
# ============================================================
"""
In your main() function, replace:

    # Calculate returns using database prices
    returns_df = calculate_underlying_returns(underlying_df)

With:

    # Calculate returns using Yahoo Finance (longer history for HMM training)
    returns_df = calculate_underlying_returns_yfinance(
        symbols=SYMBOLS,
        start_date='2016-01-01',  # Much longer training history!
        end_date=df_clean['asofdate'].max().strftime('%Y-%m-%d')
    )

This gives you 8+ years of training data instead of just ~1 year from your options database.
"""


def plot_regime_parameters(hmm_model_data, symbols=None):
    """
    Plot regime parameters (drift vs volatility) for each symbol
    Shows a 2D scatter of all 4 regimes with their Œº and œÉ
    """
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots

    regime_params = hmm_model_data['regime_params']
    regime_labels = hmm_model_data['regime_labels']

    # Colors for each regime
    regime_colors = {
        'Bull/High-Vol': '#d62728',  # Red
        'Bull/Low-Vol': '#2ca02c',  # Green
        'Bear/High-Vol': '#ff7f0e',  # Orange
        'Bear/Low-Vol': '#1f77b4',  # Blue
    }

    # Create scatter plot of regime parameters
    fig = go.Figure()

    for idx, params in regime_params.items():
        label = regime_labels[idx]
        color = regime_colors.get(label, 'gray')

        fig.add_trace(go.Scatter(
            x=[params['sigma']],
            y=[params['mu']],
            mode='markers+text',
            name=label,
            marker=dict(
                size=30 + params['n_observations'] / 500,  # Size by observations
                color=color,
                line=dict(width=2, color='white')
            ),
            text=[f"R{idx}"],
            textposition="middle center",
            textfont=dict(size=12, color='white'),
            hovertemplate=(
                f"<b>{label}</b><br>"
                f"Drift (Œº): {params['mu']:.4f} ({params['mu'] * 100:.2f}%/yr)<br>"
                f"Volatility (œÉ): {params['sigma']:.4f} ({params['sigma'] * 100:.2f}%/yr)<br>"
                f"Jump Intensity (Œª): {params['lambda']:.2f}/yr<br>"
                f"Observations: {params['n_observations']:,}<br>"
                "<extra></extra>"
            )
        ))

    # Add quadrant lines at medians
    mu_vals = [p['mu'] for p in regime_params.values()]
    sigma_vals = [p['sigma'] for p in regime_params.values()]

    median_mu = np.median(mu_vals)
    median_sigma = np.median(sigma_vals)

    # Vertical line at median sigma
    fig.add_vline(x=median_sigma, line_dash="dash", line_color="gray", opacity=0.5)
    # Horizontal line at median mu
    fig.add_hline(y=median_mu, line_dash="dash", line_color="gray", opacity=0.5)

    # Add quadrant labels
    fig.add_annotation(x=max(sigma_vals) * 0.9, y=max(mu_vals) * 0.9,
                       text="Bull/High-Vol", showarrow=False, font=dict(size=14, color='#d62728'))
    fig.add_annotation(x=min(sigma_vals) * 1.1, y=max(mu_vals) * 0.9,
                       text="Bull/Low-Vol", showarrow=False, font=dict(size=14, color='#2ca02c'))
    fig.add_annotation(x=max(sigma_vals) * 0.9, y=min(mu_vals) * 1.1,
                       text="Bear/High-Vol", showarrow=False, font=dict(size=14, color='#ff7f0e'))
    fig.add_annotation(x=min(sigma_vals) * 1.1, y=min(mu_vals) * 1.1,
                       text="Bear/Low-Vol", showarrow=False, font=dict(size=14, color='#1f77b4'))

    fig.update_layout(
        title="<b>HMM Regime Parameters</b><br>Drift (Œº) vs Volatility (œÉ) ‚Äî Bubble size = # observations",
        xaxis_title="Volatility (œÉ) ‚Äî Annualized",
        yaxis_title="Drift (Œº) ‚Äî Annualized",
        height=600,
        width=800,
        template="plotly_dark",
        showlegend=True,
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5)
    )

    fig.show()
    return fig


def plot_regime_parameters_by_symbol(returns_df, features_df, detector, symbols=None):
    """
    Plot realized drift vs volatility for each symbol, colored by regime
    Shows how each symbol behaves across different regimes
    """
    import plotly.graph_objects as go
    from plotly.subplots import make_subplots

    if symbols is None:
        symbols = features_df['underlying_symbol'].unique()

    regime_colors = {
        'Bull/High-Vol': '#d62728',
        'Bull/Low-Vol': '#2ca02c',
        'Bear/High-Vol': '#ff7f0e',
        'Bear/Low-Vol': '#1f77b4',
    }

    # Create subplots - one per symbol
    n_cols = 4
    n_rows = (len(symbols) + n_cols - 1) // n_cols

    fig = make_subplots(
        rows=n_rows, cols=n_cols,
        subplot_titles=[f"<b>{sym}</b>" for sym in symbols],
        horizontal_spacing=0.08,
        vertical_spacing=0.12
    )

    for i, symbol in enumerate(symbols):
        row = i // n_cols + 1
        col = i % n_cols + 1

        sym_features = features_df[features_df['underlying_symbol'] == symbol].copy()

        if len(sym_features) == 0:
            continue

        # Plot each regime
        for regime_idx in sym_features['regime'].unique():
            regime_data = sym_features[sym_features['regime'] == regime_idx]
            label = detector.regime_labels.get(regime_idx, f"R{regime_idx}")
            color = regime_colors.get(label, 'gray')

            # Calculate regime-specific stats for this symbol
            mu = regime_data['log_return'].mean() * 252
            sigma = regime_data['realized_vol'].mean()

            fig.add_trace(
                go.Scatter(
                    x=[sigma],
                    y=[mu],
                    mode='markers',
                    name=label if i == 0 else None,  # Only show legend once
                    showlegend=(i == 0),
                    marker=dict(
                        size=15,
                        color=color,
                        line=dict(width=1, color='white')
                    ),
                    hovertemplate=(
                        f"<b>{symbol} - {label}</b><br>"
                        f"Œº: {mu:.4f}<br>"
                        f"œÉ: {sigma:.4f}<br>"
                        f"n: {len(regime_data)}<br>"
                        "<extra></extra>"
                    )
                ),
                row=row, col=col
            )

    fig.update_layout(
        title="<b>Regime Parameters by Symbol</b><br>Each point = one regime for that symbol",
        height=300 * n_rows,
        width=1200,
        template="plotly_dark",
        legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5)
    )

    # Update all axes labels
    fig.update_xaxes(title_text="Volatility (œÉ)")
    fig.update_yaxes(title_text="Drift (Œº)")

    fig.show()
    return fig

def main():
    """Execute complete pipeline"""

    print("\n" + "=" * 80)
    print("STEP 1: DATA EXTRACTION & PREPARATION")
    print("=" * 80)

    conn = connect_to_db()
    if conn is None:
        return

    try:
        limit = 100000 if SIMPLE else None
        df_raw = extract_options_data(conn, limit=limit)

        if len(df_raw) == 0:
            print("No data extracted!")
            return

        # Clean options data using Step 1 filters
        df_clean = prepare_options_data(df_raw, conn)

        if len(df_clean) == 0:
            print("No data after cleaning!")
            return

        # Define training and testing windows based on OPTIONS data range
        first_asof = pd.to_datetime(df_clean['asofdate'].min()).normalize()
        last_asof = pd.to_datetime(df_clean['asofdate'].max()).normalize()

        # Training: 2016 ‚Üí day before first options data
        # Testing: first options date ‚Üí last options date
        train_start = pd.Timestamp('2016-01-01')
        train_end = first_asof - pd.Timedelta(days=1)
        test_start = first_asof
        test_end = last_asof

        print(f"\nTRAIN/TEST SPLIT:")
        print(f"  Training: {train_start.date()} ‚Üí {train_end.date()} (pre-options history)")
        print(f"  Testing:  {test_start.date()} ‚Üí {test_end.date()} (options period)")

        # Download returns from Yahoo Finance covering BOTH periods (2016 ‚Üí present)
        returns_df = calculate_underlying_returns_yfinance(
            symbols=['SPY', 'QQQ', 'IWM', 'AAPL', 'MSFT', 'TSLA', 'XOM', 'JPM'],
            start_date='2016-01-01',
            end_date=last_asof.strftime('%Y-%m-%d')
        )

        # CRITICAL FIX: Use YFinance prices for underlying_df (full history!)
        # This ensures visualizations show 2016 ‚Üí present, not just options period
        underlying_df = returns_df[['asofdate', 'underlying_symbol', 'underlying_price']].drop_duplicates()

        print(f"\n‚úì Underlying prices loaded: {underlying_df['asofdate'].min().date()} ‚Üí {underlying_df['asofdate'].max().date()}")

        # Split returns data
        train_returns = returns_df[returns_df['asofdate'] <= train_end].copy()
        test_returns = returns_df[returns_df['asofdate'] >= test_start].copy()

        # Options data is all testing
        train_options = pd.DataFrame()
        test_options = df_clean.copy()

        print("\n" + "FINAL DATA SPLIT SUMMARY".center(80, "="))
        print(f"   Training returns: {train_returns['asofdate'].min().date()} ‚Üí {train_returns['asofdate'].max().date()}")
        print(f"   Testing returns : {test_returns['asofdate'].min().date()} ‚Üí {test_returns['asofdate'].max().date()}")
        print(f"   Underlying prices: {underlying_df['asofdate'].min().date()} ‚Üí {underlying_df['asofdate'].max().date()}")
        print(f"   Training options: {len(train_options):,} contracts (none - pre-options period)")
        print(f"   Testing options : {len(test_options):,} contracts (100% out-of-sample)")
        print("=" * 80)

    finally:
        conn.close()

    # Continue with HMM regime detection...
    # [Rest of your HMM code remains exactly the same]

    # Continue with HMM regime detection using only option-derived data...
    # [Rest of your HMM code remains the same]

        # ===================================================================
        # STEP 2: HMM REGIME DETECTION ‚Äî CORRECTED VERSION
        # ===================================================================
        print("\n" + "=" * 80)
        print("STEP 2: HMM REGIME DETECTION (trained 2016 ‚Üí train_end)")
        print("=" * 80)

        # Train HMM on training period only
        results, optimal_window = sensitivity_analysis(train_returns, WINDOW_LENGTHS, N_REGIMES)

        optimal_result = results[optimal_window]
        detector = optimal_result['detector']
        features_df = optimal_result['features_df']  # This is TRAINING features with 'regime' column

        print(
            f"\n   Training features dates: {features_df['asofdate'].min().date()} ‚Üí {features_df['asofdate'].max().date()}")
        print(
            f"   Features observations  : {len(features_df):,} across {features_df['underlying_symbol'].nunique()} symbols")

        # Show transition matrix
        print("\nTransition Matrix (trained on pre-options data):")
        trans_matrix = detector.get_transition_matrix()
        regime_labels = [detector.regime_labels.get(i, f'R{i}') for i in range(N_REGIMES)]
        trans_df = pd.DataFrame(trans_matrix, index=regime_labels, columns=regime_labels)
        print(trans_df.round(4))

        # ===================================================================
        # TRAINING PERIOD DIAGNOSTICS
        # ===================================================================
        print("\n" + "=" * 80)
        print("üîç REGIME DISTRIBUTION CHECK")
        print("=" * 80)
        print("\nTRAINING PERIOD (in-sample):")
        train_regime_dist = features_df['regime'].value_counts().sort_index()
        for regime_idx, count in train_regime_dist.items():
            label = detector.regime_labels.get(regime_idx, f"R{regime_idx}")
            print(f"  {label:20} {count:,} observations ({100 * count / len(features_df):.1f}%)")

        # ===================================================================
        # üîß FIX: PROPER OUT-OF-SAMPLE REGIME PREDICTION
        # ===================================================================
        print("\n" + "=" * 80)
        print("PREDICTING TEST PERIOD REGIMES (out-of-sample)")
        print("=" * 80)

        # Use FULL returns_df for rolling window calculation
        print(f"\nUsing full return history for rolling windows...")
        print(f"   Full data range: {returns_df['asofdate'].min().date()} ‚Üí {returns_df['asofdate'].max().date()}")

        # Prepare features on FULL history (includes pre-test data for windows)
        all_features = detector.prepare_features(returns_df, optimal_window)

        print(f"‚úì Generated features for full period: {len(all_features):,} observations")
        print(
            f"   Feature date range: {all_features['asofdate'].min().date()} ‚Üí {all_features['asofdate'].max().date()}")

        # Split AFTER feature calculation
        train_features_full = all_features[all_features['asofdate'] <= train_end].copy()
        test_features_full = all_features[all_features['asofdate'] >= test_start].copy()

        print(f"\nAfter split:")
        print(f"   Train features: {len(train_features_full):,} obs")
        print(f"   Test features:  {len(test_features_full):,} obs")

        if len(test_features_full) == 0:
            raise ValueError("No test features generated! Check date ranges.")

        # ===================================================================
        # PREDICT REGIMES FOR BOTH PERIODS
        # ===================================================================

        # Get training statistics for standardization
        X_train_raw = np.column_stack([
            train_features_full['log_return'].values,
            train_features_full['realized_vol'].values
        ])
        X_train_mean = X_train_raw.mean(axis=0)
        X_train_std = X_train_raw.std(axis=0)
        X_train_std[X_train_std == 0] = 1.0

        print(f"\nTraining feature statistics:")
        print(f"   Return mean: {X_train_mean[0]:.6f}, std: {X_train_std[0]:.6f}")
        print(f"   Vol mean:    {X_train_mean[1]:.6f}, std: {X_train_std[1]:.6f}")

        # Predict TRAINING regimes (should match features_df, but ensures consistency)
        X_train_scaled = (X_train_raw - X_train_mean) / X_train_std
        train_regimes = detector.model.predict(X_train_scaled)
        train_features_full['regime'] = train_regimes

        # Predict TEST regimes using same scaling
        X_test_raw = np.column_stack([
            test_features_full['log_return'].values,
            test_features_full['realized_vol'].values
        ])
        X_test_scaled = (X_test_raw - X_train_mean) / X_train_std
        test_regimes = detector.model.predict(X_test_scaled)
        test_features_full['regime'] = test_regimes

        # ===================================================================
        # TEST PERIOD DIAGNOSTICS
        # ===================================================================
        print("\nTEST PERIOD (out-of-sample prediction):")
        test_regime_dist = pd.Series(test_regimes).value_counts().sort_index()

        for regime_idx in range(N_REGIMES):
            count = test_regime_dist.get(regime_idx, 0)
            label = detector.regime_labels.get(regime_idx, f"R{regime_idx}")
            pct = 100 * count / len(test_regimes) if len(test_regimes) > 0 else 0
            print(f"  {label:20} {count:,} observations ({pct:.1f}%)")

        regime_changes = (pd.Series(test_regimes) != pd.Series(test_regimes).shift(1)).sum()
        print(f"\n  Regime switches in test period: {regime_changes}")

        if test_regime_dist.nunique() == 1:
            print("\n  ‚ö†Ô∏è  WARNING: All test data classified as single regime!")
        elif test_regime_dist.nunique() < N_REGIMES:
            print(f"\n  ‚ö†Ô∏è  WARNING: Only {test_regime_dist.nunique()}/{N_REGIMES} regimes observed")
        else:
            print(f"\n  ‚úì Good: All {N_REGIMES} regimes observed in test period")

        # ===================================================================
        # PER-SYMBOL DIAGNOSTICS
        # ===================================================================
        print("\n" + "=" * 80)
        print("üîç TEST PERIOD REGIME DISTRIBUTION BY SYMBOL")
        print("=" * 80)

        for symbol in sorted(test_features_full['underlying_symbol'].unique()):
            symbol_test = test_features_full[test_features_full['underlying_symbol'] == symbol]
            print(f"\n{symbol}: ({len(symbol_test)} days)")

            regime_counts = symbol_test['regime'].value_counts().sort_index()
            for regime_idx in range(N_REGIMES):
                count = regime_counts.get(regime_idx, 0)
                label = detector.regime_labels.get(regime_idx, f"R{regime_idx}")
                pct = 100 * count / len(symbol_test) if len(symbol_test) > 0 else 0
                print(f"  {label:20} {count:4} days ({pct:5.1f}%)")

        # ===================================================================
        # PER-MONTH DIAGNOSTICS
        # ===================================================================
        print("\n" + "=" * 80)
        print("üîç REGIME DISTRIBUTION BY MONTH (TEST PERIOD)")
        print("=" * 80)

        test_features_full['month'] = test_features_full['asofdate'].dt.to_period('M')
        for month in sorted(test_features_full['month'].unique()):
            month_data = test_features_full[test_features_full['month'] == month]
            print(f"\n{month}:")
            regime_counts = month_data['regime'].value_counts().sort_index()
            for regime_idx in range(N_REGIMES):
                count = regime_counts.get(regime_idx, 0)
                label = detector.regime_labels.get(regime_idx, f"R{regime_idx}")
                pct = 100 * count / len(month_data) if len(month_data) > 0 else 0
                print(f"  {label:20} {count:3} obs ({pct:5.1f}%)")

        # ===================================================================
        # VISUALIZATION
        # ===================================================================
        print("\nGenerating TRAINING PERIOD visualization...")
        visualize_regimes(features_df, detector, underlying_df, optimal_window, train_end_date=train_end)

        # ===================================================================
        # SAVE MODEL WITH FULL REGIME HISTORY
        # ===================================================================
        print("\n" + "=" * 80)
        print("SAVING HMM MODEL WITH FULL REGIME HISTORY")
        print("=" * 80)

        # NOW both train_features_full and test_features_full have 'regime' columns!
        full_regime_sequence = pd.concat([
            train_features_full[['asofdate', 'underlying_symbol', 'regime']],
            test_features_full[['asofdate', 'underlying_symbol', 'regime']]
        ], ignore_index=True).drop_duplicates(subset=['asofdate', 'underlying_symbol'])

        print(f"‚úì Full regime sequence: {len(full_regime_sequence):,} observations")
        print(
            f"   Date range: {full_regime_sequence['asofdate'].min().date()} ‚Üí {full_regime_sequence['asofdate'].max().date()}")

        save_data = {
            'optimal_window': optimal_window,
            'n_regimes': N_REGIMES,
            'regime_params': detector.regime_params,
            'regime_labels': detector.regime_labels,
            'transition_matrix': trans_matrix,
            'regime_sequence': full_regime_sequence,
            'hmm_model': detector.model,
            'feature_scaling': {
                'mean': X_train_mean,
                'std': X_train_std
            },
            'model_metadata': {
                'trained_on': f"{train_start.date()} ‚Üí {train_end.date()}",
                'test_period': f"{test_start.date()} ‚Üí {test_end.date()}",
                'first_options_date': str(first_asof.date()),
                'log_likelihood': detector.model.monitor_.history[-1],
                'feature_window': optimal_window,
                'train_regime_distribution': train_regime_dist.to_dict(),
                'test_regime_distribution': test_regime_dist.to_dict()
            }
        }

        with open('hmm_regime_model.pkl', 'wb') as f:
            pickle.dump(save_data, f)

        print("‚úì HMM model saved ‚Üí hmm_regime_model.pkl")

        # Continue with visualizations...
        print("\nGenerating regime parameter visualizations...")
        plot_regime_parameters(save_data)
        plot_regime_parameters_by_symbol(returns_df, features_df, detector)

        # Full history plots
        print("\n" + "=" * 80)
        print("GENERATING FULL HISTORY REGIME PLOTS (with Train/Test split)")
        print("=" * 80)

        visualize_historical_regimes(save_data, underlying_df, symbol='SPY', train_end_date=train_end)

        for sym in ['QQQ', 'IWM', 'AAPL', 'TSLA', 'MSFT', 'XOM', 'JPM']:
            if sym in underlying_df['underlying_symbol'].unique():
                visualize_historical_regimes(save_data, underlying_df, symbol=sym, train_end_date=train_end)

        print("\n" + "=" * 80)
        print("PIPELINE COMPLETED SUCCESSFULLY")
        print("=" * 80)

        return save_data


if __name__ == "__main__":
    results = main()

    # Demonstrate how to load and use the saved model
    print("\n" + "=" * 80)
    print("EXAMPLE: Loading and Using Saved HMM Model")
    print("=" * 80)
    print("""
# To use the saved model in future scripts (e.g., Step 3, 4, 5...):

import pickle

# Load the model
with open('hmm_regime_model.pkl', 'rb') as f:
    hmm_data = pickle.load(f)

# Access regime parameters for each regime
regime_params = hmm_data['regime_params']
# Example: regime_params[0] = {'mu': 0.15, 'sigma': 0.20, 'lambda': 5.2}


# Access transition matrix
transition_matrix = hmm_data['transition_matrix']
# 4x4 matrix where P[i,j] = probability of moving from regime i to j

# Access regime sequence (historical classifications)
regime_sequence = hmm_data['regime_sequence']
# DataFrame with columns: ['asofdate', 'underlying_symbol', 'regime']

# Predict regime for new data
from step2_hmm_regime_detection import get_current_regime
regime_idx, regime_label, probs = get_current_regime(
    hmm_data, 
    recent_returns=0.002,  # 0.2% daily return
    recent_vol=0.25        # 25% annualized vol
)
    """)